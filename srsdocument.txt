Voice-Enabled AI Chat Assistant – SRS and Design
Software Requirements Specification (SRS)
1. Introduction
Purpose: The purpose of this project is to develop a voice-enabled chat assistant web application (using React) that allows a user to interact with an AI (similar to ChatGPT/Claude) through voice and text. The system will continuously listen to the user’s voice (and optionally another speaker’s voice) and transcribe the conversation, enabling the AI to provide helpful answers or suggestions. It integrates audio input (microphone) and an optional camera feed to capture context (snapshots), and provides a chat interface for displaying the conversation. This SRS outlines the functional and non-functional requirements for the system.
Scope: The system is intended for personal use on a local network (single-user scenario, no multi-user support). It will function as a real-time Q&A assistant: the user can speak questions or have someone ask them questions, and by pressing a button, the system will generate an AI-driven answer based on the recorded context. The application will run in a web browser on a mobile device or PC, using local resources and free API services (no paid cloud services required in the design). There is no authentication or user management (since only one user will use it). The system will store conversation history so that the user can review past Q&A interactions, even after refreshing or restarting the app.
Definitions, Acronyms, Abbreviations:
- STT: Speech-to-Text (voice recognition, converting spoken audio to text).
- AI: The artificial intelligence backend that generates answers (e.g., a large language model such as GPT).
- Transcript: The text result of speech recognition (could be the user’s spoken words or another speaker’s).
- Context Cut: A user action (via a button) that indicates a break in the conversation; the system will treat the preceding collected transcripts as the context for an AI query.
- Discard: A user action to ignore the last captured transcript segment (not send it to AI).
- Snapshot: An image captured from the camera feed at a given moment (used as visual context or indication).
2. Overall Description
Product Perspective: This application is a standalone interactive assistant. It functions similarly to voice assistants or chatbots but with a focus on continuous listening and on-demand querying. The UI is inspired by chat applications (displaying user and assistant messages in a threaded format)[1]. Unlike conventional voice assistants that rely on wake-words, this system listens continuously and uses physical buttons for control. The system will leverage existing technologies for speech recognition and AI: for example, using the browser’s built-in Web Speech API for real-time transcription[2] and a free-tier or open-source AI model for generating responses. The product is not part of a larger system; it is a self-contained web app with an optional connection to a Wi-Fi camera.
User Classes and Characteristics: There is essentially one user role: the End User, who is the person seeking answers. The end user interacts with the system via voice and the on-screen buttons. In some scenarios, a second person (a Questioner) may be present who asks questions to the user – the system will capture both voices, but it is the end user who operates the controls and receives the AI’s suggestions. The end user is assumed to have basic technical ability to run a local web app on a device with microphone (and possibly camera). The user values quick responses and a simple interface.
Operating Environment: The software will run in a modern web browser (with support for Web Speech and media APIs) on a mobile device or PC. It is optimized for mobile use (touch input and possibly using phone hardware buttons). The device must have a microphone and optionally a camera. If an external Wi-Fi camera is used, it should be connected to the same local network, and the system should know its stream URL or IP. The environment is assumed to have at least intermittent internet access (for calling AI APIs or cloud speech recognition if needed), but the design prefers local or on-device processing when possible for speed and privacy. All components (browser client and any server) will be running in a local development mode setup, within a single trusted network (no cloud deployment).
Assumptions and Dependencies:
- The user’s browser supports required web APIs: speech recognition (e.g. Chrome’s SpeechRecognition), media device access, etc. (The Web Speech API typically uses an online service by default to perform recognition[3], so internet might be needed unless on-device speech API is available.)
- For AI responses, the system depends on a free AI service or model. It could use an API like OpenAI’s GPT-3.5 (with free trial or local deployment) or an open-source model. It assumes API keys or models are available in the development environment.
- The device’s microphone will be active continuously; we assume the user grants the page permission to use the microphone (and camera if applicable). The browser must be run in secure context (HTTPS or localhost) for these APIs to work[4].
- An optional dependency is the external camera feed: if the user chooses to use a Wi-Fi camera, it should provide a streaming URL or snapshot URL (e.g., an IP camera with MJPEG or RTSP stream). The system assumes network connectivity to that camera. If unavailable, the system will still function with audio only.
- The system assumes sufficient processing power on the local device (or local server) to handle real-time transcription and prompt/response handling. If using heavy models locally, a capable machine may be needed, but since we target swift performance, we lean on lightweight or external services.
Constraints:
- Real-time performance: There is a hard constraint that the voice transcription and AI response should feel near-instantaneous or as fast as possible. This affects design decisions like using efficient APIs and possibly caching or preprocessing. Network latency should be minimized (hence local network usage and possibly local models).
- Power and resource usage: Continuous microphone listening and possibly camera streaming can be resource intensive, especially on a mobile device (battery drain, CPU usage). The design must be mindful – e.g., possibly pause listening when not needed, or use efficient streaming. However, by requirement the system should “always keep camera and audio listening mode” on for instant capture, so we assume the user understands the battery trade-off.
- No UI login/registration: The system cannot have a login screen. It must load directly into the chat interface. Security is not a primary concern since it’s single-user on a local network, but we should still ensure API keys or sensitive config are not exposed on the client side (e.g., route API calls through a backend).
- Memory persistence: The app should retain chat history even if the page is refreshed. This constrains us to implement client-side storage (like LocalStorage/IndexedDB) or a lightweight database on the backend to save the transcripts and responses.
- Mobile-friendly UI: The interface must be usable on a mobile touchscreen. That implies using responsive design, large buttons, and possibly accommodating a physical button (volume down) press as an event. Note: Standard web pages have limited ability to detect hardware button presses (especially volume keys, which are usually reserved by the OS). This might require a workaround (e.g., a native wrapper or PWA installation to intercept, or simply an alternate on-screen control). We assume it’s acceptable that on some platforms the volume button trigger may not work and the user can use on-screen tap instead.
3. Functional Requirements
Below are the key functional requirements of the system, organized by feature. Each requirement is phrased as “The system shall…” to indicate a capability of the software.
3.1 Voice Input and Transcription:
- FR1: Continuous Listening: The system shall continuously listen to the environment through the device microphone and transcribe any speech it detects into text in real time[2]. This transcription should happen with minimal delay, using a speech recognition engine (e.g., the Web Speech API’s SpeechRecognition interface). The transcription service should handle continuous or long-form speech (not just one-shot commands), so that ongoing conversation is captured.
- FR2: Dual-Speaker Transcription: The system shall capture spoken words from both the user and any other speaker (e.g., a questioner). In the transcript log, it should record all spoken input. If possible, the system should distinguish between the user’s speech and the other person’s speech (this could be via simple turn-taking assumption or more advanced speaker identification). If accurate speaker separation is not feasible, the system will at least capture everything as a combined transcript. (The purpose is to have the full context of a Q&A exchange.)
- FR3: Voice Activation Control: The system shall provide a means to start or pause the transcription via user action. Specifically, the user can tap an on-screen button to toggle the listening state. On mobile devices, pressing a designated hardware button (e.g., the volume-down key) should also trigger the same toggle – for example, pressing the volume-down could act as a “push-to-talk” or a “cut” signal. Note: Implementing hardware button capture may depend on platform capabilities; if it’s not possible in a regular web page, this requirement could be met via an alternative approach (such as using a PWA or a React Native version).
- FR4: Transcription Display: As the system transcribes speech, it shall display the interim and final text in the chat interface in real time (e.g., show partial sentence as user speaks, then finalize it). This allows the user to see what has been captured (and verify accuracy). The transcript from the user/questioner side will appear as one side of the conversation (e.g., right-aligned bubble labeled as user or similar).
3.2 Chat Interface and AI Interaction:
- FR5: Chat Interface Layout: The system shall present a chat-style UI: a scrollable history of message bubbles, with the user’s (and questioner’s) transcribed inputs and the AI’s responses. User (human) messages should be visually distinct (for instance, on the right side, different color), and AI responses on the left[1]. There should be an input area or controls at the bottom (for example, a microphone icon for voice, a text box for typing, and the action buttons).
- FR6: Text Input Option: In addition to voice, the system shall allow the user to enter queries via text (typing) as an alternative. This covers the “only text” mode if the user prefers not to speak. In this mode, the user can type a question into an input box and submit it, and it will appear in the chat history and be sent to the AI just like a spoken query. (The system should still continuously listen unless the user explicitly mutes it, but if the user is actively typing, their speech might be ignored or paused to avoid confusion.)
- FR7: AI Query Trigger (Context Cut): The system shall provide a “Cut” button (or similarly labeled) that the user presses when they want to submit the latest question/context to the AI. The idea is that the microphone may be continuously capturing a conversation, but the AI shouldn’t respond to every word; instead, the user decides when a question has been asked and triggers the AI at that point. When the user taps the Cut button (or presses a physical button mapped to it), the system will take the transcripts captured since the last cut (i.e. the most recent question or exchange) as the query context for the AI. The system shall then send this collected text (the question and any relevant preceding dialogue) to the AI service to get an answer.
- FR8: AI Response Generation: Upon receiving a query trigger, the system shall call an AI backend (via API or local model) with the necessary context. It should include the latest question and also some previous context if needed for clarity (for example, if the conversation has multiple turns, the last few exchanges can be sent so the AI has context). The AI will then generate an answer. The system shall display the AI’s answer in the chat interface as a new message from the “assistant”. The response should appear promptly once received. (If the AI supports streaming tokens, the system could display the answer progressively for speed, but this is an enhancement.) The content of the answer should help the user answer the questioner or solve the query asked. For example, if the questioner asked, “What is the capital of France?”, the AI might respond with “The capital of France is Paris.” and this appears on screen for the user.
- FR9: AI Response via Voice (Optional): Optional/Enhancement: The system may read out the AI’s answer using text-to-speech. This was not explicitly requested (the user said “only text” in one point, implying perhaps no spoken output), so by default we will NOT implement TTS. The focus is on text display. The user can simply read the answer and convey it to the questioner themselves.
3.3 Context Management Controls:
- FR10: Discard Transcript: The system shall provide a “Discard” button. When pressed, this will discard the currently captured transcript segment (since the last cut) and not send it to the AI. This is useful if, for example, the user or someone said something that is not actually a question or not relevant, and the user doesn’t want it considered. Pressing Discard essentially clears the current unsubmitted text buffer. The system should then resume listening for new speech for the next query segment.
- FR11: Continuous Context vs Cut: By design, the system should only send the AI the text up to the last “cut” trigger, not every word spoken. That means if people keep talking without the user pressing the Cut button, the system will keep appending to the current transcript buffer (possibly showing it live but not querying the AI yet). When the user finally presses Cut, that whole buffer is sent as context. After an AI response, the system can start a new buffer for the next query. This mechanism ensures the AI answers with the full context of the question asked, rather than bits of partial conversation. The system shall not automatically send transcripts to the AI without the user’s action – this gives the user control to include only relevant context for the AI and to avoid confusion.
3.4 Camera Integration:
- FR12: Camera Feed Option (Local or Remote): The system shall support integrating a camera feed for visual context. The user can choose either the device’s built-in camera or an external Wi-Fi camera feed. There will be a setting or toggle to select camera source: e.g., “Use local camera” or “Use network camera (enter URL)”. If local camera is selected, the web app will request camera access and display the live video or at least indicate it’s on. If a network camera is used, the system will attempt to connect to it (for example, by fetching a snapshot or streaming feed from a provided URL).
- FR13: Snapshot on Query: Whenever the user triggers a “Cut” (AI query), and a camera feed is active, the system shall capture a snapshot image from the camera at that moment. This snapshot could be saved or associated with the query for context. (For instance, if the user is asking about something in their environment, an image could be useful for them or the AI. However, unless the AI model has vision capabilities, the image may just be stored for the user’s reference.) At minimum, the snapshot can be shown in the chat history or saved for later review. If a future AI model that accepts images is used, this snapshot could be sent as part of the query.
- FR14: Camera Status Indicator: The UI shall indicate the status of the camera feed. If the camera is connected and active, show a green indicator or the live feed thumbnail. If the camera is not connected or fails, show a red indicator or an error icon. The user should be able to know that the camera context is unavailable (and the system will then rely on audio only). This addresses the requirement “If camera is not connected don’t worry, just show it as red and keep audio context at least.” The system will continue to function with just audio transcripts in such a case.
- FR15: Always-On Mode: If the camera feed is enabled, the system shall continuously fetch the video (or periodically refresh if it’s a snapshot-only feed) so that it’s effectively “always on.” Similarly, the microphone is always on listening. This ensures that whenever a query trigger happens, the latest audio and visual context is ready. (This is subject to device performance limits; if needed, the video feed might be processed at a lower frame rate or paused when not needed to save resources, but from a functional standpoint it’s considered continuously monitoring.)
3.5 Session Management and Persistence:
- FR16: Conversation History Display: The chat interface shall display the ongoing conversation history: all the past transcripts and AI answers in order. This allows the user to scroll up and review previous Q&A pairs. Each entry should be labeled or visually clear who it came from (user/questioner vs assistant).
- FR17: Persistent Storage of Chats: The system shall persist the chat history so that it is not lost on a page refresh or restart. In local single-user mode, this can be achieved by saving the conversation to the browser’s local storage or indexed DB, or by having a small local database on the backend. For example, the app can save the messages array in localStorage as JSON[5]. Whenever the app loads, it will retrieve and display the last session’s messages. This ensures continuity (“whenever we refresh we need to see updated ones”).
- FR18: Multiple Sessions (Optional): Optional enhancement: The system might allow separation of conversations (like threads), but the user’s request suggests keeping all “current and previous chats” accessible. This could simply mean one running log. If needed, we might implement a way to start a fresh chat (clearing context) while still storing the old one somewhere. However, given “Only I will be using,” it may be acceptable that the app just maintains one ever-growing chat log. (We will assume a single session log for now, due to simplicity.)
3.6 Error Handling and Feedback:
- FR19: Speech Recognition Feedback: The system shall provide real-time feedback that it’s listening (e.g., an icon or animation) and possibly when it’s processing. If speech recognition fails or times out, the system should notify the user (e.g., “No speech heard, please try again”).
- FR20: AI Response Errors: If the AI API call fails (network error or no response), the system shall handle it gracefully: show an error message in the chat (“Assistant could not respond, check connection or try again”) and allow retrying.
- FR21: Camera Errors: If the camera feed cannot be accessed (permission denied, or network camera unreachable), the system shall display a message or red status (as per FR14) but continue operation with voice only.
- FR22: Performance Adjustments: If continuous listening or camera streaming is causing noticeable lag, the system may automatically adjust (e.g., it might lower video quality or momentarily pause listening during the AI response generation to save CPU). While not directly requested, this kind of adaptive behavior ensures the primary functionality (fast Q&A) remains optimal.
4. External Interface Requirements
4.1 User Interface: The UI is a crucial component for usability. The main interface is a single-page chat view which includes:
•	A message area (chat log) occupying most of the screen, showing past interactions in chronological order. Each message bubble is either aligned right (user’s or captured question) or left (AI’s answer), with different background colors for distinction[1]. For example, user/questioner transcripts might appear in a blue bubble on the right labeled “Me” (or no label if assuming single user), and AI responses in a grey bubble on the left labeled “Assistant.”
•	A floating microphone indicator or status icon that shows when audio is being recorded. Possibly an animation (like a pulsating dot or equalizer bars) to indicate live listening.
•	Two prominent buttons for control: “Cut (Submit)” and “Discard.” On mobile, these could be large touch-friendly buttons at the bottom of the screen. They might be iconified (e.g., a scissors icon for Cut, a trash can for Discard) with labels. The Cut button triggers sending the query; the Discard clears the current input buffer.
•	An input text box (with a “Send” arrow) for optional text input mode. The microphone may be automatically disabled when the user is typing, to avoid mixing inputs. The user can either speak and then press Cut, or type and press Send – both result in a query to AI.
•	A camera view or indicator: If using the local camera, a small preview window (thumbnail or background video) might be shown on screen so the user knows what’s being captured. If using a remote camera, perhaps a placeholder image or an icon indicating it’s active. If the camera is off or disconnected, the icon turns red or a “no camera” symbol appears.
•	The design should be responsive: on mobile screens, the elements fit vertically (the chat log scrolls, input and buttons anchored at bottom). On a desktop, it could show a wider layout but it’s primarily optimized for phone use (“mobile” was specified).
•	No separate pages for login or settings (to keep it simple). If needed, settings (like selecting camera source or entering an IP cam URL) can be a modal or a small menu.
4.2 Hardware Interfaces:
- Microphone: The application uses the device’s microphone via the browser. It relies on the hardware microphone of the phone or PC. If the user presses a hardware key (e.g., a Bluetooth push-to-talk button or the phone’s volume), that hardware event should be captured (within browser limits) to trigger software actions. For instance, on Android Chrome, pressing volume down might not be detectable by the web app (since it usually adjusts volume). This might require using an alternative (maybe a Bluetooth remote or a wired headset button that registers as a key event). This is a limitation to note; the system will primarily rely on on-screen tap for broad compatibility.
- Camera: If using the device camera, it interfaces through getUserMedia (webcam). This requires a camera hardware on the device. If an external IP camera is used, that camera is a separate hardware interface accessible over network (the web app or backend will fetch images from it). The system might need the IP camera’s snapshot URL or to use a library to decode an RTSP stream (likely handled in backend or via a library like JSMpeg in front). These are implementation details; from the SRS perspective, the interface is that the system can pull images from an external camera hardware given proper address.
- Device Sensors (Possible extension): Not explicitly required, but if needed, the system might use other sensors; for example, if running as a native app, it could use a hardware button or an LED. For now, the primary hardware interactions are mic and cam.
4.3 Software Interfaces:
- Speech Recognition API: The web app will use the browser’s speech recognition API (Web Speech API). This interface involves starting a SpeechRecognition instance, handling events for results, errors, etc.[2]. The SRS expects this to function as intended on supported browsers. If the browser doesn’t support it, an alternative could be an external speech-to-text API (like Google Cloud Speech via REST, or an offline library like Vosk). But those require internet or additional integration. The baseline is the Web Speech API for simplicity and cost (it’s free in-browser, though may use vendor servers behind the scenes)[3].
- AI Model API: The system will communicate with an AI language model through an API endpoint. For example, if using OpenAI’s API, the backend will call the /v1/chat/completions endpoint with the conversation history. If using a local model via a library, the interface could be a function call. In any case, there’s a software interface where the system provides text input and gets text output (the assistant’s answer). This API requires internet if using a cloud service or sufficient local resources if running a model. The design assumes one or the other is available in the development setup. We will use a backend component to keep API keys secret and to potentially swap out models as needed.
- Database/Storage: If a backend is used, it may interface with a local database (like an embedded SQLite, or even a JSON file store) to save chat history. Alternatively, if purely front-end, it interfaces with window.localStorage Web API to store data. Either way, the software interface abstracts saving and loading past messages. This is internal to our app, but it’s worth noting that a library or API (like IndexedDB’s API or a Node.js file system API) will be used for persistence.
- Network Camera API: For external camera, the software must fetch images/video from it. This could be as simple as an HTTP GET to a known URL that returns a JPEG snapshot (many IP cameras provide http://<ip>/snapshot.jpg or similar). Or it could involve opening a WebSocket or RTSP stream. We might use a library (e.g., node-rtsp-stream on the backend to convert RTSP to MJPEG over WebSocket)[6][7]. The SRS doesn’t mandate how, just that the interface exists to retrieve frames. The system should handle the communication (including perhaps credentials for the camera if needed).
4.4 Communications Interfaces:
- The app’s front-end and back-end (if a back-end server is used) will communicate over HTTP/HTTPS, likely using a REST API. For instance, the front-end might POST /api/ask with the current query (transcript text, and maybe a recent image or a flag if image is available), and the server returns the AI answer. Similarly, a request like GET /api/history could fetch stored chats on load. All such communication happens over the local network (or localhost) since it’s a local deployment. - If the AI service is cloud-based (OpenAI, Azure, etc.), the back-end server will communicate with those external endpoints using HTTPS. - Real-time considerations: for streaming transcripts or responses, WebSockets could be used (e.g., stream STT results or partial AI answer). However, initial implementation might not need streaming beyond what the Web Speech API does internally. A standard request-response cycle is acceptable given one user. - The system should use JSON as the data format for exchanging messages between front-end, back-end, and AI API (e.g., JSON payload with conversation). For example, the OpenAI Chat API expects a list of message objects with roles “user” and “assistant”[8]. Our system will compose that from the stored history: it will label transcripts as user messages and AI outputs as assistant messages to send in the API call, enabling the AI to maintain context[8].
5. Non-Functional Requirements
5.1 Performance: The application must operate in near real-time. Transcription of speech should happen within a second or two of speaking, and answers from the AI should ideally return within a few seconds (assuming the question is reasonably short). The overall question-to-answer latency is expected to be low (target under ~5 seconds depending on network). To achieve this, we plan to use efficient services: the Web Speech API recognition runs partially on-device or on a fast cloud service provided by the browser[3], and the AI calls will be optimized (possibly using a fast model). Caching strategies will be used – for example, keep the AI context in memory and only append new queries, rather than re-sending the entire history unnecessarily, to reduce payload. The design might also use an in-memory cache for active session data (transcripts, etc.) to avoid slow disk or database reads during a conversation[9].
5.2 Scalability: Scalability is not a primary concern (single user, local deployment). However, the design should allow easy switching to a more powerful backend if needed (for example, swapping the free model with a more powerful one, or enabling multiple devices to connect to the same back-end). We ensure modularity so that components like speech recognition, AI service, camera feed are decoupled and can be replaced or upgraded independently.
5.3 Reliability: The system should be reliable in continuous operation. It should handle extended periods of listening and multiple query cycles without crashing or memory leaks. If an external service fails (e.g., AI API down), the system should notify the user and not freeze. Because it’s local, we assume network reliability is under the user’s control (local Wi-Fi). The system should be able to recover from brief network interruptions (e.g., if Wi-Fi drops and comes back, the app should resume listening and be able to query again). Chat history persistence ensures that even if the browser is accidentally closed or crashes, the conversation is not lost.
5.4 Usability: Usability is critical since the user will be interacting in potentially high-pressure situations (answering a questioner quickly). The UI must be intuitive: one tap to get an answer, one tap to discard mistakes. The interface will be kept uncluttered – mainly showing the conversation and two main buttons. Any additional settings (like choosing camera) will be hidden behind an icon (gear or menu) to not confuse the main flow. Shortcuts like the physical button trigger (if working) improve ergonomics by allowing the user to trigger the AI without looking at the screen (for example, user could discretely press a phone button in pocket). The text in chat should be legible with clear contrast. The app will use simple language for any prompts or errors. No login means no user onboarding friction. Overall, it should “just work” when opened.
5.5 Maintainability & Extensibility: As a local dev project, it should be easy to maintain by the developer. We will structure code modularly (separating front-end components and backend services). If new features are to be added (e.g., adding text-to-speech for answers, or integrating another sensor), the design should allow adding new modules without major refactor. Using React on the front-end encourages component-based structure, and a Node/Express backend can expose new endpoints for new features easily. The code will be documented and use clear class/variable naming.
5.6 Security: Since this is local-use and only one user, security concerns are minimal. However, there are a few points: the microphone and camera access are sensitive, so the app runs on HTTPS/localhost to satisfy browser security[4]. We ensure that any API keys (for AI services) are kept on the server side and not exposed in client code. If the system stores conversation history, it should be stored in a location only accessible by the user (in-browser storage or a local file). No personal data beyond conversation content is stored. If using an external camera feed that requires credentials, those should be stored securely (not plaintext in client). For the scope of local dev, we can assume the environment is trusted.
5.7 Portability: The app is web-based and should run on any modern browser (Chrome, Firefox, Safari, Edge) that supports the required APIs. Mobile browsers (especially Android Chrome) should work for voice; iOS Safari’s support for Web Speech API is limited (Apple requires using SiriKit or a workaround), so the best experience might be on Android or using a cross-platform framework. Optionally, packaging the app as a hybrid app (using Cordova or Capacitor) could enable deeper integration (like capturing the volume button). That is outside this scope but the design does not preclude it. The backend (if any) will run on any platform supporting Node.js (if we choose Node) or on Python/Flask etc., as long as the environment can access microphone if needed (for server-side STT, though we plan client-side STT). Since we focus on local network deployment, we won’t deploy to cloud, but if needed, the server could be containerized in Docker for portability.
5.8 Ethical/Privacy considerations: As this system continuously listens and possibly watches, it’s important to clarify that data is not sent to third parties beyond what’s necessary for functionality. The design should, if extended beyond personal use, include a push-to-talk mode or wake-word to avoid always recording audio (for privacy), but since the requirement explicitly asks for always-listening mode (likely for convenience), we assume the user consents to that. None of the audio or images are stored long-term or transmitted to cloud except the parts needed to answer queries (transcripts and possibly images if that feature were used with an AI). For now, our AI model is Q&A text-based, so images might not be transmitted out.
Software Design
In this section, we outline the high-level architecture and the main components (classes/modules) of the system. The design uses a client-server architecture: a React front-end for the UI and interactive features, and a backend server (e.g., in Node.js) to handle AI API calls, and possibly to interface with an external camera feed or manage persistent storage. This separation keeps the front-end lightweight (especially important for mobile performance) and secures any private API keys or heavy computations on the back-end. However, for a purely local single-user deployment, it’s also possible to run everything on the client (using browser APIs and calling third-party services directly). We present a design that can accommodate either approach, focusing on modular components.
1. Architectural Overview
Data Flow: The typical flow of data in the system is as follows: 1. Voice Input: The user (and/or questioner) speaks. The client’s Speech Recognition module captures audio via the microphone and produces text transcripts in real-time[2]. These transcripts are immediately displayed in the chat UI so the user can see them. They are also stored in a Transcript Buffer (memory) on the client.
2. User Triggers Query: When the user presses the Cut/Submit button, the client takes the text from the transcript buffer (since the last cut) – which presumably contains the question that needs answering – and packages it into a request. This request may include additional context: recent prior messages (to maintain conversation continuity) and possibly a camera snapshot. The request is sent to the back-end AI Service (via an API call).
3. AI Processing: The back-end AI Service (or possibly directly in client if using a client-accessible API) receives the query. It formats the conversation history into the proper prompt format required by the AI model (for instance, as a list of messages with roles[8]). Then it calls the AI model (through an API call to an external service or a local inference engine). The AI processes the input and returns a generated answer.
4. Response Delivery: The back-end sends the AI’s answer back to the client. The client receives it and appends this as a new message in the chat UI (from the Assistant). The transcript buffer is cleared (ready for new input). The conversation history (both the user’s query and the AI answer) is saved (locally or via backend) for persistence.
5. Continuous Listening: Meanwhile, unless the user hit a pause or the app is intentionally stopped, the microphone resumes capturing any further speech into a new transcript buffer. The user may press Discard to clear it if something irrelevant was picked up, or press Cut again for the next question, repeating the cycle.
6. Camera Snapshots: If the camera is active, when a Cut occurs, the Camera module captures the current frame (image). That image could be displayed in the chat (for user reference: e.g., “Image captured at 10:30:45”) and/or encoded and sent to the back-end. (If the AI were vision-capable, it could analyze it, but our initial design might not process the image with AI, since GPT-3.5, for example, can’t directly process images. However, it could be stored or shown for completeness). If not sending to AI, the image is just stored alongside the chat (like an attachment). The camera continues streaming so it’s ready for the next capture.
High-Level Components: The system can be divided into the following major components: - UI Layer (React Components): Presents the chat log, buttons, and indicators to the user. Key components here might be ChatWindow, MessageList, MessageItem, InputControls, etc. This layer invokes functionality from the control layer (like starting/stopping listening, sending queries).
- Control/Logic Layer (Client-side): This includes the modules managing audio and camera on the client: - SpeechRecognizer (or a custom hook in React) that wraps the Web Speech API to start/stop recognition and handle transcript events. It updates the UI with interim results.
- AudioController which uses SpeechRecognizer and ties it to UI controls (e.g., pressing Cut will temporarily stop listening while sending the query, then resume). It might also manage state like “is listening or not”.
- CameraController that manages access to the camera. For a local camera, it uses navigator.mediaDevices.getUserMedia() to get a video stream[10]. For a remote camera, it may call a backend endpoint that returns a snapshot. It provides a method to “getSnapshot()” which the logic calls on a Cut event.
- ChatManager on the client side, which manages the array of messages (the conversation state). It interfaces with local storage for saving/loading history, and provides methods to add a new user message or assistant message to the state. It also holds the current transcript buffer (which is essentially a draft user message being built from audio).
- These client-side controllers will call the Backend API when needed (e.g., send query). They use fetch/Axios to communicate with server endpoints.
- Back-end Layer: If we include a back-end, the main components are:
- AIService (could be part of an Express server): responsible for receiving query requests from client, adding necessary context, and forwarding to the AI model API. For OpenAI, this means constructing the messages with roles and calling the OpenAI API with the API key[11]. For a local model, it could call a function. It then returns the answer to the client. This service could also implement simple rate-limiting or caching (if the same question is asked frequently, though that’s a niche optimization).
- StorageService: handles storing chat history on the server side (if not solely using client storage). For instance, it could use a small database or file. It might expose endpoints like /loadHistory and /saveMessage. However, to keep things fast and simple, we might actually let the client manage history persistence (via localStorage) instead of round-tripping every message to server. In design, both options are possible; the localStorage approach is simpler and since it’s single-user, it’s acceptable and extremely fast (no network needed for loading history)[5].
- CameraProxy (optional): If using an IP camera with RTSP, the browser can’t natively handle RTSP. We might run a proxy on the server that converts the camera’s stream to a format usable by the browser (MJPEG over WebSocket, as shown in one solution[6][7]). This component would fetch frames from the camera upon request or continuously and serve them. For a snapshot-over-HTTP type camera, the client could fetch directly without needing a proxy (just need the URL and perhaps CORS enabled). In design, we acknowledge the need for handling different camera interfaces.
State Management: We will maintain state such as “listening vs not listening”, “current transcript text”, “chat history”, and “camera on/off” using React state (for UI updates) and context where needed. For example, ChatManager might be implemented as a React Context provider that any component can tap into to get the latest messages.
Error flows: If voice recognition stops (e.g., due to silence or error), the SpeechRecognizer should automatically restart it (unless the user pressed Cut or explicitly paused). This ensures continuous capture. If the user is in the middle of speaking when they press Cut, we might finalize whatever is transcribed up to that point for the query. The system should also flush the recognition results on pressing Cut (some implementations of Web Speech API require stopping the recognition to get final results). After sending to AI, it can start a new recognition session. These transitions are managed in the AudioController logic.
2. Key Classes/Modules Design
Below we list the main classes/modules in the design, along with their responsibilities, methods, and interactions. (In an actual implementation, some of these might be React functional components or hooks rather than classes, but we describe them in an object-oriented way for clarity.)
•	SpeechRecognizer Module:
•	Purpose: Capture microphone audio and convert to text.
•	Implementation: Wraps the Web Speech API’s SpeechRecognition.
•	Key Methods/Functions: startListening(), stopListening(), onResult(callback) (register a callback for handling transcripts).
•	Behavior: When started, it streams voice input to the speech service. As results come in (interim and final), it invokes callbacks or dispatches events. It handles continuous mode: once a result is finalized, it can keep listening for new speech (the Web Speech API has a continuous flag and often you must restart it after a pause).
•	Uses: This is used by the AudioController. It updates the ChatManager’s current transcript text as the user speaks.
•	Note: Browser’s recognition is typically cloud-backed, meaning audio is sent to a server and text returned[3]. The user must grant mic permission.
•	AudioController Class:
•	Purpose: Manage the overall audio recording session and user controls related to audio.
•	Key State: listening (bool), and possibly a reference to current partial transcript text.
•	Key Methods: toggleListening() – bound to the screen tap or volume button to start/stop listening. In always-on scenario, this might not be frequently used, because we plan to always listen; but perhaps the user can manually pause listening if they want.
•	handleCutAction() – what happens when user presses the Cut button. This will stop the SpeechRecognizer (to finalize input), retrieve the final transcript buffer from ChatManager, then call ChatManager.finalizeUserMessage() to create a message entry. It then calls the Backend API (AIService) to get a response. After sending, it may re-enable listening (start a fresh recognition) for new input.
•	handleDiscardAction() – clears the current transcript buffer (via ChatManager) and possibly resets the SpeechRecognizer state. It does not send anything to AI. It can immediately resume listening for new speech.
•	Uses: SpeechRecognizer, ChatManager, AI Client. It orchestrates them.
•	Interaction: For example, user presses Cut -> AudioController.handleCut -> SpeechRecognizer.stop -> ChatManager.getBufferText -> send to AI -> receive answer -> ChatManager.addMessage(answer) -> UI updates -> SpeechRecognizer.start (for next round).
•	ChatManager Class:
•	Purpose: Maintain the conversation history and current transcript buffer.
•	State: messages (array of Message objects, where a Message might have {sender: "user"|"assistant"|"system", text: "...", timestamp: ..., maybe image: ...}), and currentBuffer (text that’s been transcribed but not yet finalized into a user message). Possibly also a lastCutIndex to know from where to slice context.
•	Key Methods:
o	addUserTranscript(text, isFinal) – called continuously by SpeechRecognizer as text comes in. If isFinal is false, update currentBuffer (interim text). If true (final end of utterance), also update currentBuffer but mark that segment as final (in UI maybe move it from italic to normal). (We might not push to messages array yet because we wait for user to hit Cut to finalize it as a user message in history.)
o	finalizeUserMessage() – when the user triggers Cut, this takes whatever is in currentBuffer (the question asked), creates a new Message object for it (with sender “user”), appends to messages array, and clears the currentBuffer. It returns the text (for sending to AI). It might also include recent context messages if needed for AI.
o	addAssistantMessage(text) – append a new Message with sender “assistant” and given text to messages.
o	saveHistory() – writes the messages array to persistent storage. Could be called after each new message or periodically. In a browser, this uses localStorage: e.g., localStorage.setItem('chat_messages', JSON.stringify(messages))[5]. On a backend, this might write to a DB.
o	loadHistory() – loads messages from storage (on startup) to initialize the chat.
•	Uses: Storage (LocalStorage or backend API), provides data to UI (the UI will observe messages state, e.g., via React useState/useEffect to rerender when a new message is added).
•	Note: It also interfaces with the camera images: if a snapshot is taken, ChatManager might create a special message type or attachment to include the image (e.g., a Message with image: blob or text: [Image]). The UI can render that accordingly.
•	CameraController Class:
•	Purpose: Manage camera connection and snapshot retrieval.
•	Key Methods: startCamera(source) – where source could be "local" or a URL for remote. If local, it calls navigator.mediaDevices.getUserMedia({ video: true }) to get a MediaStream[10], then sets up a video element or canvas in the UI to show it (or not shown if not needed). If remote, it might do nothing on client and just store the URL or inform the backend to prepare.
•	getSnapshot() – if local camera: grabs a frame from the video stream (using a canvas drawImage and toBlob). If remote camera: perhaps call a backend endpoint like /camera/snapshot which returns an image from the IP camera. This method returns the image data (blob or base64).
•	Key State: cameraActive (bool), sourceType (local/remote), maybe videoElement reference for local preview.
•	Interaction: On Cut action, AudioController could call CameraController.getSnapshot() and then include that image in the query. For initial version, the image might just be added to chat via ChatManager (for user reference) but not sent to AI (since AI might not use it). In future, if AI can use it, the AIService would need to accept image input.
•	Error handling: If camera fails to start or snapshot fails, it should throw an error or return null. AudioController/ChatManager then knows camera context is not available and just proceeds with text.
•	Backend API (Express) and AIService: (if using a Node backend)
•	The Express server will define routes like POST /api/query. This is handled by a controller function that we call AIService.queryAI(context).
•	AIService (could be a class or just encapsulated logic):
o	Purpose: Interface with the external AI model.
o	Method: generateResponse(userMessage, recentMessages) – takes the latest user question and some recent history, constructs the prompt. For example, using OpenAI’s chat format, it might do:

 	messagesForAPI = [];
for each msg in recentMessages:
    messagesForAPI.push({ role: msg.senderRole, content: msg.text });
messagesForAPI.push({ role: "user", content: userMessage });
call OpenAIChatAPI(messagesForAPI);
 	This ensures the AI gets conversation context[8]. The OpenAI API will return an assistant role message. The service extracts the text and returns it.
o	If using a different model (local), similar approach but calling the local model’s generate function.
o	It may also do minor post-processing, e.g., ensure the response text is safe or trimmed.
•	Performance: The AIService could be enhanced with caching: e.g., if the exact same userMessage was seen recently in the same session, it might reuse the previous answer to save time. However, that’s low priority unless the model is very slow.
•	Note on Multi-turn context: We might send the entire conversation history every time to the API for full context, but that could grow and slow down. A compromise is to send a sliding window of the last N messages (N could be, say, 10)[8]. The ChatManager can help by providing recentMessages() for this.
•	Camera in AI: If we had an AI that can handle images (say GPT-4 vision or a custom vision model), the AIService would also handle attaching the image. For OpenAI, maybe upload to cloud storage and provide a link, or if the API supports base64 images (some do). But since not required now, we skip implementation.
•	StorageService (Backend or Client):
•	If on backend: could be a simple module that uses a JSON file or database. E.g., using an in-memory store that dumps to file on exit (since single user). Because the user specifically said “only local dev, use free version DB/cache,” an SQLite database is a good choice (free, serverless). Alternatively, since the usage is light, even writing to a text file (like a log of chat) is fine.
•	If on client: localStorage as described earlier[5]. In that case, the ChatManager directly uses it (so no separate class needed for storage; it’s one-liner calls).
•	For a robust design, we mention that on backend we could incorporate a caching layer like Redis for active session (to get O(1) lookups) and a persistent DB for long-term[9]. But given one user, the overhead isn’t needed.
Class Interaction Summary: When the user speaks and then taps “Cut”, here’s a sequence with classes: 1. During Speaking: SpeechRecognizer produces interim text “What is the capital of Fra…”. It calls ChatManager.addUserTranscript("What is the capital of Fra", false) repeatedly, updating currentBuffer. Once the user pauses, final result “What is the capital of France?” comes, SpeechRecognizer calls ChatManager.addUserTranscript("What is the capital of France?", true). The UI now shows “What is the capital of France?” as a complete message (perhaps still awaiting user confirmation).
2. User taps Cut: AudioController.handleCutAction() triggers. It calls SpeechRecognizer.stopListening(). Then gets text = ChatManager.finalizeUserMessage(), which moves “What is the capital of France?” from the buffer into the messages list as a user message. Chat UI may visually mark it as sent.
- AudioController calls CameraController.getSnapshot(). Suppose it returns an image (say imgData). AudioController gives that to ChatManager to create an “image message” (or attaches to the user message context). For now, maybe it creates a system message like “[Snapshot captured]” with the image.
- AudioController then calls an API: BackendAPI.sendQuery({ text: "What is the capital of France?", image: imgData, history: last 2 messages }).
3. On Backend: Express receives /api/query. It calls AIService.generateResponse(userMessage="What is the capital of France?", recentMessages=[...]). The AIService calls the model API. Let’s say it returns “The capital of France is Paris.”.
- The Express handler sends JSON { answer: "The capital of France is Paris." } back.
4. Client receives answer: The AudioController (or a promise callback from fetch) gets the data. It calls ChatManager.addAssistantMessage("The capital of France is Paris."). This updates the messages list. The UI now shows the assistant’s answer. Optionally, the SpeechRecognizer could be restarted here for new input.
- ChatManager calls saveHistory() behind the scenes to persist the updated conversation.
5. The user sees the answer and can speak the next question or end the session. If next question, it’s already listening (since we resume listening immediately), so the cycle repeats.
3. Technology Choices and Considerations
To meet the requirement of being “swift and fastest”, we propose the following tech stack and optimizations:
•	Front-end: React (with possibly Redux or Context for state). We will use modern React (functional components with hooks) for simplicity. For styling, a lightweight approach like Tailwind CSS or simple CSS modules can be used to keep UI responsive and fast. React will efficiently update the parts of UI that change (new messages, live transcript text). The front-end will be built and served as a static bundle (if using a backend, the backend can serve it on /).
•	Speech API: As noted, using the Web Speech API is ideal on Chrome/Edge as it’s free and doesn’t require integrating external SDKs[12]. We should be aware that on some browsers (Safari), this might not work, so an alternative could be needed (there are third-party JS libraries that use cloud STT, but they might cost). Given this is local/dev, we assume using Chrome is acceptable to the user. The continuous listening mode of Web Speech will give us live transcription[13]. We’ll set SpeechRecognition.continuous = true and handle onspeechend by restarting to avoid it stopping after each phrase.
•	AI Backend: If using OpenAI’s GPT-3.5, it’s not actually free but often comes with free trial credit. Alternatively, since local is okay, we might consider an open-source model like GPT4All or LLaMA 2 run locally. However, running those in real-time on a mobile or low-end PC might be slow. A compromise: use the OpenAI API (or Azure OpenAI as in the example[14]) for quality and ease, which requires internet. If strictly no paid API, a smaller local model (like GPT-J or Vicuna-7B) could be hosted on the backend (maybe using a library like llamacpp or transformers in Python). This is a design detail; the SRS user did not demand offline AI, just “free version”. Free trial or local open-source qualifies. The backend is designed to be swappable in this regard.
•	Backend language: We choose Node.js with Express for the server because the front-end is in JS, and using Node allows sharing code structures if needed and is generally fast for I/O. Node can easily call external APIs and serve the React app. It’s also quick to develop, aligning with “swift” development. Performance-wise, Node can handle our single-user asynchronous calls easily (the heavy lift of AI is on the AI service, not on Node’s CPU). If maximum performance was needed, a compiled language (Go, Rust) could be used for the server to shave off some milliseconds, but that’s over-optimizing in this context.
•	Database: For persistence on server (if we do it server-side), SQLite is a good choice – it’s file-based, no server needed, and fast for the small volumes (a few KB of text). It can easily store a table of messages with columns (id, sender, text, timestamp). Alternatively, simply writing the entire chat log to a JSON file whenever it updates is acceptable (low risk of race conditions with one user, and the file can be loaded on startup). Using a heavy DB like MySQL/Postgres would be overkill. On the client side, using localStorage is effectively a small key-value database in the browser and is instantaneous for our scale[5].
•	Caching: We mention caching in terms of storing session in memory to avoid repeated DB reads[9]. Concretely, ChatManager already keeps everything in a JS object (which is memory) and only occasionally hits disk. That itself is caching. If we had a scenario where the user’s chat became very long, searching through it might become slow, but that’s not a big issue here. If we did integrate a heavy local model, caching responses for identical questions or using a smaller model for quick queries could be considered, but that’s beyond our immediate needs.
•	Camera Feed: If local camera, the browser handling it is fine (MediaStream). If network camera, Node might need to translate it. We could use FFmpeg via a Node library to pull frames from RTSP streams, but that adds complexity. An easier approach: many IP cameras have an HTTP snapshot URL – if so, the React app could fetch that image URL directly (the camera might require basic auth; we can embed that in the URL or use a proxy to hide credentials). For the design, we’ll assume either a snapshot URL is available or the camera feed can be accessible via HTML video tag if it’s MJPEG. This is somewhat specific to the hardware; the system design is flexible to support it by having CameraController either use getUserMedia or fetch from backend.
•	Optimizations for speed:
•	Use web APIs which are implemented in optimized native code (Web Speech, WebRTC for camera) for heavy tasks.
•	Keep messages short in context for AI (maybe limit to last N lines as mentioned) to reduce latency in AI processing.
•	Use asynchronous non-blocking code everywhere (which is natural in JS).
•	Possibly use Service Workers to cache static assets so the app loads extremely fast on refresh (especially if as PWA).
•	The UI can use virtualization if chat gets long (render only last some messages in DOM) to keep it snappy.
•	For development simplicity, we might not implement all of these, but the design allows them.
4. Example Scenarios
To illustrate, here are two usage scenarios and how the system behaves:
Scenario A (Single user asking AI): The user is alone and wants to use the assistant like a voice-based ChatGPT. They open the app on their phone, grant mic (and maybe camera) permission. The interface shows an empty chat and a mic indicator listening. The user asks, “What’s the weather today?” The speech recognizer captures this, displays “What’s the weather today?” The user taps the screen (Cut). The query goes to AI, which responds with “Today it’s sunny and 75°F.” This appears on screen. The user can then tap a button to speak the next query or just start talking (since auto-resume listening picks up: “How about tomorrow?”). The system sees new text, possibly concatenated with the previous if they didn’t press cut immediately. The user presses Cut after saying it. The AI gets the context (“Today it’s sunny...”, user asks “How about tomorrow?”), and replies accordingly. The history is stored so if the user refreshes, they see those Q&As.
Scenario B (User with a questioner in conversation): Suppose the user is in a meeting and someone (the questioner) asks them, “Can you explain how photosynthesis works?” The user doesn’t immediately know a concise answer. The system, running on the user’s laptop, has been quietly transcribing everything. So it has the questioner’s line “Can you explain how photosynthesis works?” recorded. The user presses a hidden Bluetooth remote (mapped to Cut) or clicks the button. The system takes that question transcription and sends it to the AI. The AI returns an explanation. The user sees on their screen a detailed answer. The user then verbally answers the questioner using that information. If the questioner asks a follow-up, the system continues to listen and capture it, enabling the user to get further help. In this mode, both “mine and his transcripts” are being collected – essentially the entire dialogue – but the AI is consulted when the user triggers it, usually right after the questioner’s part. This scenario shows how the assistant can be used as a real-time support tool. The camera in this scenario might capture the whiteboard or slides in the room, for example, but unless the AI can use that, it may not directly influence the answer (though conceivably, a future enhancement could attempt to parse text from an image or something). The system at least logs the image so the user can see the context of when that question was asked (maybe useful later).
5. Conclusion
The above SRS and design provide a comprehensive plan for building the requested voice-activated Q&A assistant. By leveraging React for a snappy UI, Web Speech API for free real-time transcription, and careful use of local resources and caching, the system will be swift and responsive as required. We have outlined how the classes and components interact to fulfill each stated requirement. This design ensures that the application will be quick (audio and visual data are processed on-device or local network) and will make use of the “best” free options (e.g., on-device APIs, local storage, etc.).
In summary, the system marries the convenience of a ChatGPT-like conversational interface with voice input and optional visual context, optimized for personal, local use. All current and past chats are retained for reference, and the user is given simple but powerful controls to manage context (cut/discard) to get the most relevant and accurate answers from the AI.
________________________________________
[1] [8] [11] [14] Building a ChatGPT-Powered and Voice-Enabled Assistant using React and Express | by Gustavo Cordido | Microsoft Azure | Medium
https://medium.com/microsoftazure/building-a-chatgpt-powered-and-voice-enabled-assistant-using-react-and-express-46823d3098b0
[2] [3] Using the Web Speech API - Web APIs | MDN
https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API/Using_the_Web_Speech_API
[4] [10] MediaDevices: getUserMedia() method - Web APIs | MDN
https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia
[5] Saving dialogue | React-chatbot-kit
https://fredrikoseberg.github.io/react-chatbot-kit-docs/docs/advanced/saving-dialogue/
[6] [7] How to display IP camera feed from an RTSP url onto react app page with node js - DEV Community
https://dev.to/qobi/how-to-display-ip-camera-feed-from-an-rtsp-url-onto-react-app-page-with-node-js-516l
[9] How do you guys deal with saving and loading chat history across sessions in production? : r/LangChain
https://www.reddit.com/r/LangChain/comments/1gvgwtj/how_do_you_guys_deal_with_saving_and_loading_chat/
[12] [React] Speech to Text — how we solved speech transcription in the ...
https://medium.com/@k.lolcio/react-speech-to-text-how-we-solved-speech-transcription-in-the-tolgy-application-8515d2adc0bd
[13] Web Speech API - GitHub Pages
https://webaudio.github.io/web-speech-api/
